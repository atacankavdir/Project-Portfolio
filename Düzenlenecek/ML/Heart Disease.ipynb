{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_x Shape:  (3158, 15)\n",
      "Train_y Shape:  (3158,)\n",
      "Test_x Shape:  (1053, 15)\n",
      "Test_y Shape:  (1053,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "df = pd.read_csv('framingham.csv', encoding='latin-1')\n",
    "# Filling out missing values\n",
    "df['BPMeds'].fillna(0, inplace = True)\n",
    "df['glucose'].fillna(df.glucose.mean(), inplace = True)\n",
    "df['totChol'].fillna(df.totChol.mean(), inplace = True)\n",
    "df['education'].fillna(1, inplace = True)\n",
    "df['BMI'].fillna(df.BMI.mean(), inplace = True)\n",
    "df['heartRate'].fillna(df.heartRate.mean(), inplace = True)\n",
    "df.dropna(inplace= True)\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "\n",
    "labels = np.array(df['TenYearCHD'])\n",
    "df= df.drop('TenYearCHD', axis = 1)\n",
    "    \n",
    "train_x, test_x, train_y, test_y = train_test_split(df, labels, test_size = 0.25, random_state = 42)\n",
    "\n",
    "print (\"Train_x Shape: \",train_x.shape)\n",
    "print (\"Train_y Shape: \", train_y.shape)\n",
    "print (\"Test_x Shape: \", test_x.shape)\n",
    "print (\"Test_y Shape: \", test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble metotlardan daha iyi sonuç elde etmenin Bagging,Boosting,Voting,Stacking gibi bir kaç yolu var.\n",
    "Ben performansı yeterince yüksek olmayan 3 algoritmayı kombinleyerek \n",
    "daha iyi bir sonuç elde etmeye çalıştığım ve daha kolay olduğu için Voting kullanmaya karar verdim.\n",
    "\n",
    "Biri diğerini oluştursa da decision tree, random forest ve Gausian Naive Bayes classifierlara voting \n",
    "uygulandığında bir performans artışı olup olmadığına bakacağız.\n",
    "Decision tree ve random forest  metodlarında ciddi bir overfitting görülüyor. \n",
    "Modeli train datası için çalıştırdığımızda çok yüksek AUC score geliyor.\n",
    "Grid Search ile Tuning yapmaya çalıştım, elde ettiğim optimum sonuç bana yeterli gelmedi.\n",
    "\n",
    "Voting için ağırlık kullanmamaya karar verdim. Bir kaç farklı algoritma denedim,\n",
    "hepsinin 1 dediklerini 1 almak, fazla 1 denene bakmak gibi;\n",
    "ama sonuçta herhangi bir algoritmanın 1 dediğini 1 sayarak daha iyi bir sonuç elde ettim.\n",
    "\n",
    "Bu algoritmalar random oldukları için farklı \n",
    "denemelerde farklı sonuçlar vermeye yatkınlar.\n",
    "Ama ulaşabilecekleri maksimum açıklama güçleri veya belirli sayıda deneme sonucunda \n",
    "verdikleri sonuçlar karşılaştırıldığında sonuç üzerinde anlamlı bir fark olduğu görülebilir.\n",
    "\n",
    "     Random bir çalıştırmada elde edilen performans artışı aşağıda görülüyor:\n",
    "     Decision Tree\n",
    "     AUC Score: 0.578496583143508\n",
    "     Random Forest\n",
    "     AUC Score: 0.5618158151643345\n",
    "     GausianNB\n",
    "     AUC Score: 0.5984184835665473\n",
    "     Voting\n",
    "     AUC Score: 0.6294825903026358\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[773 105]\n",
      " [128  47]]\n",
      "Accuracy: 0.7787274453941121\n",
      "\u001b[6;30;42mAUC Score:\u001b[0m 0.574490725675236\n",
      "Precision: 0.3092105263157895\n",
      "Recall: 0.26857142857142857\n",
      "F1 Score: 0.28746177370030584\n"
     ]
    }
   ],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier(min_samples_split = 15)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf1 = clf.fit(train_x,train_y)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "pred_y = clf1.predict(test_x)\n",
    "pred_dec_tree = pred_y.copy()\n",
    "print(confusion_matrix(test_y, pred_y))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, pred_y))\n",
    "print ('\\x1b[6;30;42m' + 'AUC Score:' + '\\x1b[0m', roc_auc_score(test_y, pred_y))\n",
    "print (\"Precision:\", precision_score(test_y, pred_y))\n",
    "print (\"Recall:\", recall_score(test_y, pred_y))\n",
    "print (\"F1 Score:\", f1_score(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[836  42]\n",
      " [160  15]]\n",
      "Accuracy: 0.8081671415004749\n",
      "\u001b[6;30;42mAUC Score:\u001b[0m 0.5189391474129514\n",
      "Precision: 0.2631578947368421\n",
      "Recall: 0.08571428571428572\n",
      "F1 Score: 0.12931034482758622\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 5)\n",
    "\n",
    "rfc = rfc.fit(train_x,train_y)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "pred_y = rfc.predict(test_x)\n",
    "pred_rand_forest = pred_y.copy()\n",
    "print(confusion_matrix(test_y, pred_y))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, pred_y))\n",
    "print ('\\x1b[6;30;42m' + 'AUC Score:' + '\\x1b[0m', roc_auc_score(test_y, pred_y))\n",
    "print (\"Precision:\", precision_score(test_y, pred_y))\n",
    "print (\"Recall:\", recall_score(test_y, pred_y))\n",
    "print (\"F1 Score:\", f1_score(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gausian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[810  68]\n",
      " [127  48]]\n",
      "Accuracy: 0.8148148148148148\n",
      "\u001b[6;30;42mAUC Score:\u001b[0m 0.5984184835665473\n",
      "Precision: 0.41379310344827586\n",
      "Recall: 0.2742857142857143\n",
      "F1 Score: 0.32989690721649484\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb = gnb.fit(train_x,train_y)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "pred_y = gnb.predict(test_x)\n",
    "pred_gnb = pred_y.copy()\n",
    "print(confusion_matrix(test_y, pred_y))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, pred_y))\n",
    "print ('\\x1b[6;30;42m' + 'AUC Score:' + '\\x1b[0m', roc_auc_score(test_y, pred_y))\n",
    "print (\"Precision:\", precision_score(test_y, pred_y))\n",
    "print (\"Recall:\", recall_score(test_y, pred_y))\n",
    "print (\"F1 Score:\", f1_score(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Algoritması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[707 171]\n",
      " [100  75]]\n",
      "Accuracy: 0.7426400759734093\n",
      "\u001b[6;30;42mAUC Score:\u001b[0m 0.6169053042629353\n",
      "Precision: 0.3048780487804878\n",
      "Recall: 0.42857142857142855\n",
      "F1 Score: 0.3562945368171021\n"
     ]
    }
   ],
   "source": [
    "#sum all prediction values of all three models to take account all 1 values\n",
    "total_pred = pred_dec_tree+pred_rand_forest+pred_gnb\n",
    "\n",
    "#turn all values into binary form by turning the values which are greater then 1 to 1.\n",
    "for k in range(0,len(total_pred)):\n",
    "    if total_pred[k] >= 1:\n",
    "        total_pred[k]=1\n",
    "    elif total_pred[k]==0:\n",
    "        total_pred[k]=0\n",
    "        \n",
    "print(confusion_matrix(test_y, total_pred))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_y, total_pred))\n",
    "print ('\\x1b[6;30;42m' + 'AUC Score:' + '\\x1b[0m', roc_auc_score(test_y, total_pred))\n",
    "print (\"Precision:\", precision_score(test_y, total_pred))\n",
    "print (\"Recall:\", recall_score(test_y, total_pred))\n",
    "print (\"F1 Score:\", f1_score(test_y, total_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
